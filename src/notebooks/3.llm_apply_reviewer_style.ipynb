{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/firatgelbal/Library/Caches/pypoetry/virtualenvs/pr-review-style-imitator-aoWNOfRk-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from utils.helper import load_env\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack_integrations.components.generators.anthropic import AnthropicGenerator\n",
    "from haystack.utils import Secret\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .env file at: /Users/firatgelbal/code/gelbal/pr-review-style-imitate/.env\n",
      "load_dotenv() result: True\n"
     ]
    }
   ],
   "source": [
    "# Ignore notebook warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "load_env()\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global cache for vectorized PR data\n",
    "PR_VECTORS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_enriched_prs(repo_name: str) -> Dict:\n",
    "    \"\"\"Load enriched PRs and create lookup tables\"\"\"\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "    enriched_file = os.path.join(project_root, 'data', 'processed', repo_name, f\"{repo_name}_enriched_prs.json\")\n",
    "\n",
    "    with open(enriched_file, 'r') as f:\n",
    "        enriched_prs = json.load(f)\n",
    "\n",
    "    # Filter out confidential PRs or not classified PRs\n",
    "    non_confidential_prs = [\n",
    "        pr for pr in enriched_prs\n",
    "        if pr.get('confidentiality', {}).get('is_confidential') == False\n",
    "    ]\n",
    "\n",
    "    # Create file-based lookup\n",
    "    files_lookup = {}\n",
    "    for pr in non_confidential_prs:\n",
    "        if 'diff_content' in pr:\n",
    "            files = parse_diff_content(pr['diff_content'])\n",
    "            for file in files:\n",
    "                if file['filename'] not in files_lookup:\n",
    "                    files_lookup[file['filename']] = []\n",
    "                files_lookup[file['filename']].append(pr)\n",
    "\n",
    "    return {\n",
    "        'prs': non_confidential_prs,\n",
    "        'files_lookup': files_lookup\n",
    "    }\n",
    "\n",
    "def parse_diff_content(diff_content: str) -> List[Dict]:\n",
    "    \"\"\"Parse diff content into structured format\"\"\"\n",
    "    files = []\n",
    "    current_file = None\n",
    "    current_diff = []\n",
    "\n",
    "    for line in diff_content.split('\\n'):\n",
    "        if line.startswith('diff --git'):\n",
    "            if current_file:\n",
    "                files.append({\n",
    "                    \"filename\": current_file,\n",
    "                    \"diff\": '\\n'.join(current_diff)\n",
    "                })\n",
    "            current_file = line.split()[-1].replace('b/', '')\n",
    "            current_diff = []\n",
    "        else:\n",
    "            current_diff.append(line)\n",
    "\n",
    "    # Add the last file\n",
    "    if current_file:\n",
    "        files.append({\n",
    "            \"filename\": current_file,\n",
    "            \"diff\": '\\n'.join(current_diff)\n",
    "        })\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_by_number(repo_name: str, pr_number: int) -> Dict:\n",
    "    \"\"\"Get PR data by PR number from enriched PRs\"\"\"\n",
    "    enriched_data = load_enriched_prs(repo_name)\n",
    "\n",
    "    for pr in enriched_data['prs']:\n",
    "        # Extract PR number from html_url\n",
    "        url_pr_number = int(pr['html_url'].split('/')[-1])\n",
    "        if url_pr_number == pr_number:\n",
    "            return pr\n",
    "\n",
    "    raise ValueError(f\"PR #{pr_number} not found in enriched data\")\n",
    "\n",
    "def get_prs_by_files(reviewer_name: str, repo_name: str, file_paths: List[str], max_prs: int = 30) -> List[Dict]:\n",
    "    \"\"\"Get past PRs that modified the same files\"\"\"\n",
    "    # Load enriched PR data\n",
    "    enriched_data = load_enriched_prs(repo_name)\n",
    "\n",
    "    # Get PRs that touched any of the given files\n",
    "    relevant_prs = set()\n",
    "    for file_path in file_paths:\n",
    "        if file_path in enriched_data['files_lookup']:\n",
    "            relevant_prs.update(pr['html_url'] for pr in enriched_data['files_lookup'][file_path])\n",
    "\n",
    "    # Get full PR data and sort by recency\n",
    "    prs = [pr for pr in enriched_data['prs'] if pr['html_url'] in relevant_prs]\n",
    "    prs.sort(key=lambda x: x.get('updated_at', ''), reverse=True)\n",
    "\n",
    "    return prs[:max_prs]\n",
    "\n",
    "def get_similar_prs_by_diff(repo_name: str, current_diff: str, max_prs: int = 30) -> List[Dict]:\n",
    "    \"\"\"Find similar PRs based on diff content using cached vectorizer\"\"\"\n",
    "    global PR_VECTORS\n",
    "    enriched_data = load_enriched_prs(repo_name)\n",
    "\n",
    "    # Initialize or get cached vectors\n",
    "    if repo_name not in PR_VECTORS:\n",
    "        print(f\"Initializing PR vectors for {repo_name}...\")\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        pr_diffs = [pr.get('diff_content', '') for pr in enriched_data['prs']]\n",
    "        tfidf_matrix = vectorizer.fit_transform(pr_diffs)\n",
    "        PR_VECTORS[repo_name] = {\n",
    "            'vectorizer': vectorizer,\n",
    "            'tfidf_matrix': tfidf_matrix,\n",
    "            'pr_indices': {i: pr for i, pr in enumerate(enriched_data['prs'])}\n",
    "        }\n",
    "\n",
    "    # Get cached vectors\n",
    "    vectorizer = PR_VECTORS[repo_name]['vectorizer']\n",
    "    tfidf_matrix = PR_VECTORS[repo_name]['tfidf_matrix']\n",
    "    pr_indices = PR_VECTORS[repo_name]['pr_indices']\n",
    "\n",
    "    # Calculate similarity with current PR\n",
    "    current_vector = vectorizer.transform([current_diff])\n",
    "    similarity_scores = cosine_similarity(current_vector, tfidf_matrix)[0]\n",
    "\n",
    "    # Get most similar PRs with scores\n",
    "    similar_indices = similarity_scores.argsort()[-max_prs:][::-1]\n",
    "    similar_prs = []\n",
    "\n",
    "    for idx in similar_indices:\n",
    "        pr = pr_indices[idx].copy()  # Create a copy to add similarity score\n",
    "        pr['similarity_score'] = f\"{similarity_scores[idx]:.2f}\"\n",
    "        similar_prs.append(pr)\n",
    "\n",
    "    return similar_prs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_review_template = \"\"\"\n",
    "You are an AI code review assistant tasked with reviewing a pull request in the style of a specific reviewer. Your goal is to provide feedback that matches the reviewer's style, focus areas, and characteristics described in the instructions.\n",
    "\n",
    "Review Instructions:\n",
    "{{instructions}}\n",
    "\n",
    "Here are some recent example PRs that are related to this new PR for reference:\n",
    "{% for pr in example_prs %}\n",
    "Example PR {{loop.index}}:\n",
    "Title: {{pr.title}}\n",
    "Description: {{pr.body}}\n",
    "\n",
    "Reviews:\n",
    "{% for review in pr.reviews %}\n",
    "{% if review.user_login == pr.reviewer_login %}\n",
    "- {{review.body}}\n",
    "{% endif %}\n",
    "{% endfor %}\n",
    "\n",
    "Comments:\n",
    "{% for comment in pr.review_comments %}\n",
    "{% if comment.user_login == pr.reviewer_login %}\n",
    "- File: {{comment.path}}\n",
    "  Line: {{comment.line}}\n",
    "  Comment: {{comment.body}}\n",
    "{% endif %}\n",
    "{% endfor %}\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "When reviewing the PR below, pay special attention to:\n",
    "1. How the reviewer typically structures their feedback\n",
    "2. Common patterns in technical focus areas\n",
    "3. The reviewer's tone and communication style\n",
    "4. How they balance different types of feedback\n",
    "5. Their typical approach to code organization and patterns\n",
    "6. How they handle documentation and testing requirements\n",
    "\n",
    "Pull Request to Review:\n",
    "Title: {{pr_title}}\n",
    "Description: {{pr_description}}\n",
    "Files Changed:\n",
    "{% for file in files_changed %}\n",
    "File: {{file.filename}}\n",
    "Changes:\n",
    "```\n",
    "{{file.diff}}\n",
    "```\n",
    "{% endfor %}\n",
    "\n",
    "Please provide a comprehensive review in the reviewer's style, including both overall feedback and specific file-level comments.\n",
    "\n",
    "Output the review in this format:\n",
    "OVERALL FEEDBACK:\n",
    "(General feedback about the PR)\n",
    "\n",
    "FILE COMMENTS:\n",
    "[file_path]\n",
    "- Line X: (comment)\n",
    "- Line Y: (comment)\n",
    "\n",
    "[another_file_path]\n",
    "- Line Z: (comment)\n",
    "\"\"\"\n",
    "\n",
    "apply_review_prompt_builder = PromptBuilder(template=apply_review_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reviewer_style(reviewer_name: str, repo_name: str, pr_title: str, pr_description: str,\n",
    "                        files_changed: List[Dict], num_examples: int = 20) -> Dict[str, str]:\n",
    "    \"\"\"Apply reviewer's style to generate PR review\"\"\"\n",
    "    # Load reviewer instructions\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "    instructions_file = os.path.join(project_root, 'data', 'reviewer_instructions',\n",
    "                                   repo_name, f\"{reviewer_name}_review_instructions.txt\")\n",
    "\n",
    "    with open(instructions_file, 'r', encoding='utf-8') as f:\n",
    "        instructions = f.read()\n",
    "\n",
    "    # Get relevant PRs by file paths\n",
    "    file_paths = [f['filename'] for f in files_changed]\n",
    "    file_history_prs = get_prs_by_files(reviewer_name, repo_name, file_paths, max_prs=100)\n",
    "\n",
    "    # Get similar PRs by diff content\n",
    "    current_diff = '\\n'.join(f['diff'] for f in files_changed)\n",
    "    similar_prs = get_similar_prs_by_diff(repo_name, current_diff, max_prs=100)\n",
    "\n",
    "    # Combine and deduplicate example PRs\n",
    "    seen_pr_ids = set()\n",
    "    candidate_prs = []\n",
    "    for pr in file_history_prs + similar_prs:\n",
    "        if pr['html_url'] not in seen_pr_ids:\n",
    "            seen_pr_ids.add(pr['html_url'])\n",
    "            candidate_prs.append(pr)\n",
    "\n",
    "    # Randomly select num_examples PRs from candidates\n",
    "    example_prs = random.sample(candidate_prs, min(num_examples, len(candidate_prs)))\n",
    "\n",
    "    # Prepare prompt input\n",
    "    input_dict = {\n",
    "        \"instructions\": instructions,\n",
    "        \"example_prs\": example_prs,\n",
    "        \"pr_title\": pr_title,\n",
    "        \"pr_description\": pr_description,\n",
    "        \"files_changed\": files_changed\n",
    "    }\n",
    "\n",
    "    # Set up pipeline\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(\"prompt_builder\", apply_review_prompt_builder)\n",
    "    pipeline.add_component(\n",
    "        \"llm\",\n",
    "        AnthropicGenerator(\n",
    "            api_key=Secret.from_token(ANTHROPIC_API_KEY),\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            generation_kwargs={\n",
    "                \"max_tokens\": 8192,  # Max possible in Claude 3.5 sonnet\n",
    "                \"temperature\": 0.3\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n",
    "\n",
    "    # Run pipeline\n",
    "    result = pipeline.run(data=input_dict)\n",
    "    review_text = result[\"llm\"][\"replies\"][0]\n",
    "\n",
    "    # Parse the response into sections\n",
    "    try:\n",
    "        overall_feedback = re.search(r\"OVERALL FEEDBACK:\\s*(.*?)(?:\\n\\nFILE COMMENTS:|\\Z)\",\n",
    "                                   review_text, re.DOTALL).group(1).strip()\n",
    "        file_comments_text = re.search(r\"FILE COMMENTS:\\s*(.*?)$\", review_text, re.DOTALL)\n",
    "        file_comments = file_comments_text.group(1).strip() if file_comments_text else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM response: {e}\")\n",
    "        overall_feedback = review_text\n",
    "        file_comments = \"\"\n",
    "\n",
    "    return {\n",
    "        \"overall_feedback\": overall_feedback,\n",
    "        \"file_comments\": file_comments\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_current_changes(reviewer_name: str, repo_name: str, pr_number: int) -> Dict[str, str]:\n",
    "    \"\"\"Review PR changes using specified reviewer's style\"\"\"\n",
    "    try:\n",
    "        # Get PR data\n",
    "        pr_data = get_pr_by_number(repo_name, pr_number)\n",
    "\n",
    "        # Generate review\n",
    "        review = apply_reviewer_style(\n",
    "            reviewer_name=reviewer_name,\n",
    "            repo_name=repo_name,\n",
    "            pr_title=pr_data[\"title\"],\n",
    "            pr_description=pr_data[\"body\"],\n",
    "            files_changed=parse_diff_content(pr_data[\"diff_content\"]),\n",
    "            num_examples=50\n",
    "        )\n",
    "\n",
    "        # Save review result\n",
    "        save_review_result(repo_name, pr_number, reviewer_name, review)\n",
    "\n",
    "        print(f\"\\nGenerated review in {reviewer_name}'s style for PR #{pr_number}:\\n\")\n",
    "        print(\"Overall Feedback:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(review[\"overall_feedback\"])\n",
    "        print(\"\\nFile Comments:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(review[\"file_comments\"])\n",
    "\n",
    "        return review\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating review: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_review_result(repo_name: str, pr_number: int, reviewer_name: str, review_data: Dict):\n",
    "    \"\"\"Save the generated review to disk\"\"\"\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "    reviews_dir = os.path.join(project_root, 'data', 'generated_reviews', repo_name)\n",
    "    os.makedirs(reviews_dir, exist_ok=True)\n",
    "\n",
    "    output_file = os.path.join(reviews_dir, f\"{repo_name}_pr_{pr_number}_{reviewer_name}_review.json\")\n",
    "\n",
    "    review_metadata = {\n",
    "        'pr_number': pr_number,\n",
    "        'reviewer_name': reviewer_name,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'review': review_data\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(review_metadata, f, indent=2)\n",
    "\n",
    "    print(f\"Review saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PR vectors for nosara...\n",
      "Review saved to /Users/firatgelbal/code/gelbal/pr-review-style-imitate/data/generated_reviews/nosara/nosara_pr_9711_c00pmode_review.json\n",
      "\n",
      "Generated review in c00pmode's style for PR #9711:\n",
      "\n",
      "Overall Feedback:\n",
      "--------------------------------------------------\n",
      "💡 The addition of logging is a good practice for observability, but we could make a few improvements to make it even more useful:\n",
      "\n",
      "1. Consider adding more context around the API request lifecycle:\n",
      "   - Number of action IDs being processed\n",
      "   - Success/failure status of each request\n",
      "   - Response timing information\n",
      "\n",
      "2. The logging level seems appropriate (INFO), but we might want to consider adding DEBUG level logs for more detailed troubleshooting.\n",
      "\n",
      "3. ❓ What prompted the need for additional logging? Understanding the context might help us implement a more comprehensive solution.\n",
      "\n",
      "4. 🔍 The PR would benefit from:\n",
      "   - Documentation of the logging changes in comments\n",
      "   - A test case to verify the logging behavior\n",
      "   - Examples of the new log output in the PR description\n",
      "\n",
      "The change itself is simple and low-risk, but there's opportunity to make it more robust while we're touching this area.\n",
      "\n",
      "File Comments:\n",
      "--------------------------------------------------\n",
      "ganymedes2/affiliates/src/main/scala/com/automattic/ganymedes2/sparkt/IrApiSparkActionItemsETL.scala\n",
      "- Line 108: ❓ Consider adding more context to this log message:\n",
      "  ```scala\n",
      "  logger.info(s\"Fetching item details from API for actionId: $actionId [${actionIds.indexOf(actionId) + 1}/${actionIds.size}]\")\n",
      "  ```\n",
      "  This would help track progress through large sets of actions.\n",
      "\n",
      "- Line 109: 💡 Consider wrapping the API call in timing metrics:\n",
      "  ```scala\n",
      "  val startTime = System.currentTimeMillis()\n",
      "  val responseJson = readActionIdJson(actionId)\n",
      "  logger.info(s\"API request for actionId $actionId completed in ${System.currentTimeMillis() - startTime}ms\")\n",
      "  ```\n",
      "\n",
      "- Line 105: ⚠️ Since we're adding observability, consider adding error handling around the map operation to log failed requests:\n",
      "  ```scala\n",
      "  .map { actionId =>\n",
      "    try {\n",
      "      // existing code\n",
      "    } catch {\n",
      "      case e: Exception =>\n",
      "        logger.error(s\"Failed to process actionId: $actionId\", e)\n",
      "        throw e\n",
      "    }\n",
      "  }\n",
      "  ```\n",
      "\n",
      "The changes look good overall and can be approved once you've considered these suggestions. The critical ones are:\n",
      "1. Adding progress tracking to the log message\n",
      "2. Adding basic error handling\n",
      "\n",
      "The other suggestions are optional improvements that could be made in a follow-up PR if desired.\n",
      "\n",
      "❤️ Good initiative on improving the observability of the API requests!\n"
     ]
    }
   ],
   "source": [
    "#reviewer_name = \"gelbal\"\n",
    "# reviewer_name = \"deltaWhiskey\"\n",
    "# repo_name = \"looker\"\n",
    "# pr_number = 2823\n",
    "\n",
    "#reviewer_name = \"Khrol\" \"c00pmode\" \"anandnalya\"\n",
    "reviewer_name = \"c00pmode\"\n",
    "repo_name = \"nosara\"\n",
    "pr_number = 9711\n",
    "\n",
    "review = review_current_changes(\n",
    "    reviewer_name=reviewer_name,\n",
    "    repo_name=repo_name,\n",
    "    pr_number=pr_number\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr-review-style-imitator-aoWNOfRk-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
